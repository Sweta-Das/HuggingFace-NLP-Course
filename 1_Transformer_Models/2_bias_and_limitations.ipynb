{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of Using a Pre-trained (Fine-tuned) Models\n",
    "\n",
    "- To enable pretraining on large amounts of data, researchers often scrape all the content they can find on the internet (good and bad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(task=\"fill-mask\",\n",
    "                    model=\"bert-base-uncased\")\n",
    "\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see 2 warnings here: <br>\n",
    "1Ô∏è‚É£ First Warning (About `GenerationMixin`)<br>\n",
    "2Ô∏è‚É£ Second Warning (Unused Weights in `BertForMaskedLM`)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ About `GenerationMixin`\n",
    "\n",
    "‚ö†Ô∏è BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. \n",
    "From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
    " <br><br>\n",
    "\n",
    "- Warning indicates that `BertForMaskedLM` has a method `prepare_inputs_for_generation` i.e., used for text generation, but it doesn't inherit from `GenerationMixin`\n",
    "- Why *Warning*?\n",
    "    - `BertForMaskedLM` isn't originally designed for text generation, but it does have some generation-related functionality.\n",
    "    - In future versions (**v4.50+** of `transformers`), `PreTrainedModel` will no longer inherit `GenerationMixin` automatically, meaning **BERT models will lose the ability to generate text using `.generate()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "unmasker = pipeline(task=\"fill-mask\",\n",
    "                    model=AutoModelForMaskedLM.from_pretrained(model_name),\n",
    "                    tokenizer=AutoTokenizer.from_pretrained(model_name))\n",
    "\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Unused Weights in `BertForMaskedLM`\n",
    "\n",
    "‚ö†Ô∏è Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM:\n",
    "- ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
    "<br>\n",
    "\n",
    "- Warning indicates that **some weights from `bert-base-uncased` weren't used** when loading `BertForMaskedLM`.\n",
    "- Why *Warning*?\n",
    "    - `bert-base-uncased` was originally trained for multiple tasks, including sequence classification (e.g., next sentence prediction)\n",
    "    - The **masked language model (MLM)** only needs **part of the weights**, so the **pooling layer and next sentence prediction head are ignored.**\n",
    "- Is this an *issue*?\n",
    "    - No. This is an expected behavior when using a BERT model for `fill-mask` instead of full pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Suppress Warning messages\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "unmasker = pipeline(\n",
    "    task = \"fill-mask\",\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name),\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    ")\n",
    "\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've suppressed all the warnings related to model loading. It is safe, because **unused weights** don't affect performance, and the warning is only informational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waitress', 'nurse', 'model', 'maid', 'teacher']\n"
     ]
    }
   ],
   "source": [
    "result = unmasker(\"This girl works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['teacher', 'carpenter', 'mechanic', 'farmer', 'waiter']\n"
     ]
    }
   ],
   "source": [
    "result = unmasker(\"This boy works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender.\n",
    "\n",
    "This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it‚Äôs trained on the English Wikipedia and BookCorpus datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, when using these tools, we need to keep in mind that the original model could very easily generate sexist, racist or homophobic content. Fine-tuning the model on our data won't make this intrinsic bias disappear."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
